# Execute below commands after opening Pyspark Shell.
# Execute each command one by one.

from pyspark.sql.types import StructType,StructField, StringType, IntegerType

person_list = [("Berry","","Allen",1,"M"),
        ("Oliver","Queen","",2,"M"),
        ("Robert","","Williams",3,"M"),
        ("Tony","","Stark",4,"F"),
        ("Rajiv","Mary","Kumar",5,"F")
    ]
    
schema = StructType([ \
        StructField("firstname",StringType(),True), \
        StructField("middlename",StringType(),True), \
        StructField("lastname",StringType(),True), \
        StructField("id", IntegerType(), True), \
        StructField("gender", StringType(), True), \
      
    ])
    
 df = spark.createDataFrame(data=person_list,schema=schema)
 
 df.show(truncate=False)

# Spark and Hadoop are on same environment, so the reason we are just giving the path of HDFS, or else it should be
hdfs://namenode:8080/input_data/departments.csv

df1 = spark.read.option("header",True).csv("/input_data/departments.csv")

df1.printSchema();
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)

df2 = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/departments.csv");

df2.printSchema();
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)
