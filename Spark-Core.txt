# Execute below commands after opening Pyspark Shell.
# Execute each command one by one.

from pyspark.sql.types import StructType,StructField, StringType, IntegerType

person_list = [("Berry","","Allen",1,"M"),
        ("Oliver","Queen","",2,"M"),
        ("Robert","","Williams",3,"M"),
        ("Tony","","Stark",4,"F"),
        ("Rajiv","Mary","Kumar",5,"F")
    ]
    
schema = StructType([ \
        StructField("firstname",StringType(),True), \
        StructField("middlename",StringType(),True), \
        StructField("lastname",StringType(),True), \
        StructField("id", IntegerType(), True), \
        StructField("gender", StringType(), True), \
      
    ])
    
 df = spark.createDataFrame(data=person_list,schema=schema)
 
 df.show(truncate=False)

# Spark and Hadoop are on same environment, so the reason we are just giving the path of HDFS, or else it should be
hdfs://namenode:8080/input_data/departments.csv

df1 = spark.read.option("header",True).csv("/input_data/departments.csv")

df1.printSchema();
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)

df2 = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/departments.csv");

df2.printSchema();
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)
 
 
>>> empDF = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/employees.csv");
>>> empDF.printSchema();
root
 |-- EMPLOYEE_ID: integer (nullable = true)
 |-- FIRST_NAME: string (nullable = true)
 |-- LAST_NAME: string (nullable = true)
 |-- EMAIL: string (nullable = true)
 |-- PHONE_NUMBER: string (nullable = true)
 |-- HIRE_DATE: string (nullable = true)
 |-- JOB_ID: string (nullable = true)
 |-- SALARY: integer (nullable = true)
 |-- COMMISSION_PCT: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- DEPARTMENT_ID: integer (nullable = true)
 
 empDF.select("*")
 
 empDF.select("employee_id").show();
 
 empDF.select(empDF.FIRST_NAME, empDF.EMPLOYEE_ID).show();
 
 empDF.select(empDF["FIRST_NAME"], empDF.EMPLOYEE_ID).show();
 
from pyspark.sql.functions import col

empDF.select(col("EMPLOYEE_ID"), col("FIRST_NAME")).show();

empDF.select(col("EMPLOYEE_ID").alias("EMP_ID"), col("FIRST_NAME").alias("F_NAME")).show();

>>> from pyspark.sql.functions import col, count
>>> empDF.select(count("*")).show();

#Derive New column:
>>> empDF.withColumn("NEW_SALARY",col("SALARY") + 1000).select("EMPLOYEE_ID","FIRST_NAME","SALARY","NEW_SALARY").show();

#Update existing column values:
>>> empDF.withColumn("SALARY",col("SALARY") - 1000).select("EMPLOYEE_ID","FIRST_NAME","SALARY").show();

#Renaming the existing column:
>>> empDF.withColumnRenamed("SALARY","EMP_SALARY").show();

#Drop the column:
>>> empDF.drop("COMMISSION_PCT").show();

#Filtering:
>>> empDF.filter(col("salary") <5000).select("Employee_id","salary").show(10);

>>> empDF.filter((col("Department_id") == 50) & (col("salary") <5000)).select("Employee_id","salary","Department_id").show(10);

#SQL kind of syntax for filtering
>>> empDF.filter("Department_id == 50 and salary <5000").select("Employee_id","salary","Department_id").show(10);

#Find unique values
>>> empDF.distinct().show();

# Drop Duplicates
>>> empDF.dropDuplicates().show();

>>> empDF.dropDuplicates(["department_id","hire_date"]).select("Employee_id","Hire_date","department_id").show();

>>> from pyspark.sql.functions import *

>>> empDF.count();

>>> empDF.select(count("salary")).show()

>>> empDF.select(max("Salary").alias("max_salary")).show();

>>> empDF.select(min("Salary").alias("min_salary")).show();


>>> empDF.select("first_name","department_id","salary").orderBy("salary").show()

>>> empDF.select("first_name","department_id","salary").orderBy(col("department_id").asc(),col("salary").desc()).show()

empDF.groupBy("department_id").sum("Salary").show();

>>> empDF.groupBy("department_id").max("Salary").show();

empDF.groupBy("department_id","job_id").sum("Salary","employee_id").show();
