# Execute below commands after opening Pyspark Shell.
# Execute each command one by one.

from pyspark.sql.types import StructType,StructField, StringType, IntegerType

person_list = [("Berry","","Allen",1,"M"),
        ("Oliver","Queen","",2,"M"),
        ("Robert","","Williams",3,"M"),
        ("Tony","","Stark",4,"F"),
        ("Rajiv","Mary","Kumar",5,"F")
    ]
    
schema = StructType([ \
        StructField("firstname",StringType(),True), \
        StructField("middlename",StringType(),True), \
        StructField("lastname",StringType(),True), \
        StructField("id", IntegerType(), True), \
        StructField("gender", StringType(), True), \
      
    ])
    
 df = spark.createDataFrame(data=person_list,schema=schema)
 
 df.show(truncate=False)

# Spark and Hadoop are on same environment, so the reason we are just giving the path of HDFS, or else it should be
hdfs://namenode:8080/input_data/departments.csv

df1 = spark.read.option("header",True).csv("/input_data/departments.csv")

df1.printSchema();
root
 |-- DEPARTMENT_ID: string (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: string (nullable = true)

df2 = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/departments.csv");

df2.printSchema();
root
 |-- DEPARTMENT_ID: integer (nullable = true)
 |-- DEPARTMENT_NAME: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- LOCATION_ID: integer (nullable = true)
 
 
>>> empDF = spark.read.option("header",True).option("inferSchema",True).csv("/input_data/employees.csv");
>>> empDF.printSchema();
root
 |-- EMPLOYEE_ID: integer (nullable = true)
 |-- FIRST_NAME: string (nullable = true)
 |-- LAST_NAME: string (nullable = true)
 |-- EMAIL: string (nullable = true)
 |-- PHONE_NUMBER: string (nullable = true)
 |-- HIRE_DATE: string (nullable = true)
 |-- JOB_ID: string (nullable = true)
 |-- SALARY: integer (nullable = true)
 |-- COMMISSION_PCT: string (nullable = true)
 |-- MANAGER_ID: string (nullable = true)
 |-- DEPARTMENT_ID: integer (nullable = true)
 
 empDF.select("*")
 
 empDF.select("employee_id").show();
 
 empDF.select(empDF.FIRST_NAME, empDF.EMPLOYEE_ID).show();
 
 empDF.select(empDF["FIRST_NAME"], empDF.EMPLOYEE_ID).show();
 
from pyspark.sql.functions import col

empDF.select(col("EMPLOYEE_ID"), col("FIRST_NAME")).show();

empDF.select(col("EMPLOYEE_ID").alias("EMP_ID"), col("FIRST_NAME").alias("F_NAME")).show();

>>> from pyspark.sql.functions import col, count
>>> empDF.select(count("*")).show();

#Derive New column:
>>> empDF.withColumn("NEW_SALARY",col("SALARY") + 1000).select("EMPLOYEE_ID","FIRST_NAME","SALARY","NEW_SALARY").show();

#Update existing column values:
>>> empDF.withColumn("SALARY",col("SALARY") - 1000).select("EMPLOYEE_ID","FIRST_NAME","SALARY").show();

#Renaming the existing column:
>>> empDF.withColumnRenamed("SALARY","EMP_SALARY").show();

#Drop the column:
>>> empDF.drop("COMMISSION_PCT").show();

#Filtering:
>>> empDF.filter(col("salary") <5000).select("Employee_id","salary").show(10);

>>> empDF.filter((col("Department_id") == 50) & (col("salary") <5000)).select("Employee_id","salary","Department_id").show(10);

#SQL kind of syntax for filtering
>>> empDF.filter("Department_id == 50 and salary <5000").select("Employee_id","salary","Department_id").show(10);

#Find unique values
>>> empDF.distinct().show();

# Drop Duplicates
>>> empDF.dropDuplicates().show();

>>> empDF.dropDuplicates(["department_id","hire_date"]).select("Employee_id","Hire_date","department_id").show();

>>> from pyspark.sql.functions import *

>>> empDF.count();

>>> empDF.select(count("salary")).show()

>>> empDF.select(max("Salary").alias("max_salary")).show();

>>> empDF.select(min("Salary").alias("min_salary")).show();


>>> empDF.select("first_name","department_id","salary").orderBy("salary").show()

>>> empDF.select("first_name","department_id","salary").orderBy(col("department_id").asc(),col("salary").desc()).show()

empDF.groupBy("department_id").sum("Salary").show();

>>> empDF.groupBy("department_id").max("Salary").show();

empDF.groupBy("department_id","job_id").sum("Salary","employee_id").show();

empDF.groupBy("department_id").agg(sum("salary").alias("total_salary")).show()

empDF.groupBy("department_id").agg(sum("salary").alias("Total_salary"),max("salary").alias("Max_Salary"),min("salary").alias("Min_Salary"),avg("salary").alias("Average_Salary")).where(col("Max_Salary") >= 10000).show()


empDF.createOrReplaceTempView("employee")

spark.sql("select * from employee").show();

spark.sql("select employee_id, department_id, rank() over(partition by department_id order by salary desc) as rank_salary from employee").show()


# JOINS:
empDF.join(DeptDf,empDF.DEPARTMENT_ID == DeptDf.DEPARTMENT_ID, "inner").select(empDF.EMPLOYEE_ID, empDF.DEPARTMENT_ID, DeptDf.DEPARTMENT_NAME).show();

empDF.join(DeptDf,empDF.DEPARTMENT_ID == DeptDf.DEPARTMENT_ID, "left").select(empDF.EMPLOYEE_ID, empDF.DEPARTMENT_ID, DeptDf.DEPARTMENT_NAME).show(100);

empDF.join(DeptDf,empDF.DEPARTMENT_ID == DeptDf.DEPARTMENT_ID, "fullouter").select(empDF.EMPLOYEE_ID, empDF.DEPARTMENT_ID, DeptDf.DEPARTMENT_NAME).show(100);

empDF.alias("emp1").join(empDF.alias("emp2"), col("emp1.MANAGER_ID") == col("emp2.MANAGER_ID"), "inner").select(col("emp1.employee_id"), col("emp2.first_name")).show(100);

empDF.alias("emp1").join(empDF.alias("emp2"), col("emp1.MANAGER_ID") == col("emp2.MANAGER_ID"), "inner").select(col("emp1.employee_id"), col("emp2.first_name")).dropDuplicates().show(100);


empDF.join(DeptDf,(empDF.DEPARTMENT_ID == DeptDf.DEPARTMENT_ID) & (DeptDf.LOCATION_ID == 1700), "inner").select(empDF.EMPLOYEE_ID, empDF.DEPARTMENT_ID, DeptDf.DEPARTMENT_NAME).show(100);

location_data = [(1700,'INDIA'),(1800,'USA')]

schema = StructType([StructField("LOCATION_ID", IntegerType(), True), StructField("LOCATION_NAME", StringType(), True)])

locDF = spark.createDataFrame(data = location_data, schema = schema)

empDF.join(DeptDf,(empDF.DEPARTMENT_ID == DeptDf.DEPARTMENT_ID) & (DeptDf.LOCATION_ID == 1700), "inner").join(locDF, DeptDf.LOCATION_ID == locDF.LOCATION_ID, "inner").select(empDF.EMPLOYEE_ID, empDF.DEPARTMENT_ID, DeptDf.DEPARTMENT_NAME,locDF.LOCATION_NAME).show(100);


## UDF
def uppercase(in_str):
        out_str = in_str.upper()
        return out_str
        
upperCaseUDF = udf(lambda z:uppercase(z), StringType())

empDF.select(col("EMPLOYEE_ID"), upperCaseUDF(col("FIRST_NAME")), upperCaseUDF(col("LAST_NAME"))).show();

@udf(returnType=StringType())
... def uppercasenew(in_str):
...     out_str = in_str.upper()
...     return out_str

empDF.select(col("EMPLOYEE_ID"), uppercasenew(col("FIRST_NAME")), uppercasenew(col("LAST_NAME"))).show();

>>> from pyspark.sql.window import Window
>>> windowSpec = Window.partitionBy("DEPARTMENT_ID").orderBy(col("SALARY").desc())

>>> empDF.withColumn("salary_rank", rank().over(windowSpec)).select("DEPARTMENT_ID", "SALARY","salary_rank").show();

>>> empDF.withColumn("SUM", sum("SALARY").over(windowSpec)).select("DEPARTMENT_ID", "SALARY","SUM").show();

>>> windowSpec = Window.partitionBy("DEPARTMENT_ID")
>>> empDF.withColumn("SUM", sum("SALARY").over(windowSpec)).select("DEPARTMENT_ID", "SALARY","SUM").show();



BROADCAST JOIN
-------------------------------------------------

from pyspark.sql.functions import *

spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 104857600) --> 104 MB (104857600 in bytes)

spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1) for displaying the Broadcast Join

empDF.join(broadcast(DeptDf),empDF.DEPARTMENT_ID == DeptDf.DEPARTMENT_ID, "inner").select(empDF.EMPLOYEE_ID, empDF.DEPARTMENT_ID, DeptDf.DEPARTMENT_NAME).show();

resultdf = empDF.join(broadcast(DeptDf),empDF.DEPARTMENT_ID == DeptDf.DEPARTMENT_ID, "inner").select(empDF.EMPLOYEE_ID, empDF.DEPARTMENT_ID, DeptDf.DEPARTMENT_NAME)


WRITE OPERATION
--------------------------------------------------
resultdf.write.option("header",True).csv("output/result")

--> To Overwrite the data - resultdf.write.mode("overwrite").option("header",True).csv("output/result")
--> To append and also change the file format - resultdf.write.mode("append").option("header",True).format("json").save("output/result")


PARTITION BY
---------------------------------------------------

resultdf.write.mode("overwrite").partitionBy('DEPARTMENT_ID').option("header",True).format("json").save("output/result")
